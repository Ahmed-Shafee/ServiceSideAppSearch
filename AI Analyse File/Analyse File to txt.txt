import PyPDF2
import warnings
import textract
import pandas as pd

import nltk
import re
import string
from nltk.corpus import stopwords, brown
from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from autocorrect import spell

from autocorrect import Speller
from nltk.tokenize import word_tokenize

warnings.filterwarnings("ignore",category=DeprecationWarning)


filename = '../input/manifesto/manibook'
open_filename = open(filename, 'rb')

ind_manifesto = PyPDF2.PdfFileReader(open_filename)


ind_manifesto.getDocumentInfo()


total_pages = ind_manifesto.numPages
total_pages

count = 0
text = ''

# Lets loop through, to read each page from the pdf file
while (count < total_pages):
    # Get the specified number of pages in the document
    mani_page = ind_manifesto.getPage(count)
    # Process the next page
    count += 1
    # Extract the text from the page
    text += mani_page.extractText()

if text != '':
    text = text

else:
    textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng')



    def to_lower(text):

        """
        Converting text to lower case as in, converting "Hello" to  "hello" or "HELLO" to "hello".
        """

        # Specll check the words
        spell = Speller(lang='en')

        texts = spell(text)

        return ' '.join([w.lower() for w in word_tokenize(text)])


    lower_case = to_lower(text)
    print(lower_case)


def clean_text(lower_case):
    # split text phrases into words
    words = nltk.word_tokenize(lower_case)

    # Create a list of all the punctuations we wish to remove
    punctuations = ['.', ',', '/', '!', '?', ';', ':', '(', ')', '[', ']', '-', '_', '%']

    # Remove all the special characters
    punctuations = re.sub(r'\W', ' ', str(lower_case))

    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words
    stop_words = stopwords.words('english')

    # Getting rid of all the words that contain numbers in them
    w_num = re.sub('\w*\d\w*', '', lower_case).strip()

    # remove all single characters
    lower_case = re.sub(r'\s+[a-zA-Z]\s+', ' ', lower_case)

    # Substituting multiple spaces with single space
    lower_case = re.sub(r'\s+', ' ', lower_case, flags=re.I)

    # Removing prefixed 'b'
    lower_case = re.sub(r'^b\s+', '', lower_case)




# Lemmatize the words
wordnet_lemmatizer = WordNetLemmatizer()

lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in clean_text(lower_case)]

# lets print out the output from our function above and see how the data looks like
clean_data = ' '.join(lemmatized_word)
print(clean_data)



